# Attribution Reporting API

April 5, 2021

Meet link: [https://meet.google.com/jnn-rhxv-nsy](https://meet.google.com/jnn-rhxv-nsy)

Use Google meet “Raise hand” for queuing


# Agenda



*   Introductions
*   Scribe volunteer:
*   Other comments / feedback on [app-to-web](https://github.com/WICG/conversion-measurement-api/blob/main/app_to_web.md) proposal 
*   Aggregate MPC: [feedback on ISRG’s Prio model](https://github.com/abetterinternet/prio-documents/issues/18#issuecomment-801248636) to satisfy some of the hierarchical use-cases we discussed in a previous meeting
    *   Underlying crypto primitives open sourced ([link](https://github.com/google/distributed_point_functions))
    *   Traffic estimates
    *   
*   Issues
    *   [impressionOrigin getting access to conversion report](https://github.com/WICG/conversion-measurement-api/issues/96)
    *   [Integration-minimalistic reporting for advertisers](https://github.com/WICG/conversion-measurement-api/issues/119)
    *   [Problems with 64 bit ID](https://github.com/WICG/conversion-measurement-api/issues/123)
*   Any other business


# Attendees — please sign yourself in!



1. Charlie Harrison (Chrome)
2. Brian May (dstillery)
3. Mikko Juola (NextRoll)
4. Larissa Licha (NextRoll)
5. Michael Kleber (Chrome)
6. John Delaney (Chrome)
7. Erik Taubeneck (Facebook)
8. Lorenzo Hernandez (NextRoll)
9. Maud Nalpas (Chrome)
10.  Viraj Awati (Amazon)
11.  Andrew Knox (Facebook)
12.  Andrew Pascoe (NextRoll)
13.  Marshall Vale (Chrome)
14.  Daniel Rojas (Chrome)
15.  
16. 


# Notes

Feedback on App to Web



*   Charlie: Still open question on how to support webviews. Currently don’t have a plan outside of publishers doing the verification within the webview.
*   Erik: Ran into similar issues. If there was some device system that would vend a nonce before ad serving, it could be included in the request.
*   Charlie: So this is looking at something which tacks onto the existing auth proposals
*   Erik: For views you can pre-empt and do the request, however for clicks you could also pre-empt. Changes the meaning of the token, as the token would be available to bad actors once the ad is served, not once it is clicked. The other option is to add a round trip.
*   Brian: Could you mitigate fraud by doing these request periodically, so bad actors would not know when the tokens are available.
*   Charlie: We were hoping to rely on the reporting origin to be in charge of determining if an event is valid. The publisher has mechanisms to controls this via permission policy etc. If we move into a model where we lose this security, like in a bad webview, we would need to start sending authentication requests to the publisher rather than a third party. Unless we had a trusted way to learn this 3P.
*   Erik: So if inside an app, we have new.example using adtech.example, there is no control across the webview boundary.Seems hard outside of changes to webview.
*   Charlie: It’s possible that willing publishers could be doing this processing themselves.
*   Erik: Could the outer app attest to the trustworthiness of the events?
*   Charlie: Can’t think of many technical enforcements.
*   Brad: One technical enforcement is to use Chrome Custom Tabs (instead of webview).
*   Charlie: Or if the webview implementation was changed to be run isolated out of process.
*   Erik: Cannot speak to why webview is used over CCT
*   Brad: If there are missing features that prevent the use of CCT, please communicate them.

Updates on MPC protocol



*   Charlie: Made some comments on ISRG protocol, who are working on prio v3. Posted a comment on the complexities of ad measurement. If prio adopted these more complex models, it could potentially be used as the underlying system
*   Charlie: Open sourced work on distributed point functions. Implementing in c++ over go gave 2x performance. There are numbers linked on the prio issue.
*   Charlie: It would be useful to understand the scale this solution would need to be at. It’s hard to estimate the scale these servers would need to be to support the ecosystem. We would appreciate any ideas on ways to estimate the number of conversions we expect to flow through such a system, the number of parties using the system, etc. Obviously some parties may not want to share this data. Chrome has a notion of page loads, which could be a rough estimate.
*   Brian: That seems like a usable approach to start.
*   Charlie: We also have the ability to measure pages which have ads on them. But these are very rough estimates still. On a purchase page, no way to know how many different parties are measuring these conversion events, or how many events there are.
*   Brian: What is the exact number you are trying to measure?
*   Charlie: The total number of attributed conversions. From there we can know how much compute power is needed to support this, and whether it is feasible. Multipliers for verification etc. Want to avoid going forward thinking we need to process a billion records a day, when it would be a trillion records.
*   Brian: Gathering feedback on tolerances for any lost data. May want multiple verifications to account for errors.
*   Charlie: For privacy reasons there are rate limits in the API. Tolerances to those as well would be useful. Trying to come up with a reasonable privacy/utility trade offs. Limiting the number of conversions a user may have in a time period may force parties to measure more valuable conversions, e.g. the privacy params may not support a conversion for every page load.
*   Charlie: Could also learn if sampling is reasonable solution. Where you only aggregate X% records received.
*   Erik: How does this match up between click and conversion side?
*   Charlie: By the time sampling would happen, the reports have already flowed through browser verification etc.
*   Erik: clean tradeoff between cost and accuracy, unless we get into a situation where getting anything costs a lot $$$.
*   John: Naive sampling doesn’t play as well when you consider receiving fake reports in a third party
*   Charlie: It’s possible to do this where the adtech server can verify the tokens in the clear, rather than the helper servers. This is different from the existing explainer, need to update.
*   Charlie:Exploring aggregate mechanisms without thresholding, however these samples are still subject to noise. Tradeoffs between accuracy and total volume. 
*   Viraj: Aggregate API batching solution... Any pointers on how the engineering solution of how accepting these events would happen.
*   Charlie: In the existing prototype, this is done via giving the helper servers a cloud storage bucket they can read from, but that is just a prototype. Hoping to iterate further.
    *   [https://github.com/google/privacy-sandbox-aggregation-service](https://github.com/google/privacy-sandbox-aggregation-service)
*   Viraj: Directional proposal is a good start. Budgeting MPC for real time ads, wouldn’t be able to do this on streaming data. Ideally we would like this budgeting at a lower level like X/XX minutes.
*   Charlie: Aggregate API is on the order of hours~, which is better than days for the event level API. Aggregate API still needs to delay reports, before any other privacy mechanisms, to ensure that timestamps can’t be used to temporally join server logs.Still need to collect enough reports in a batch to get enough data in the output.
*   Viraj: If you could separate out the delays/reasoning, and say that delay 1 is for privacy and another is the delay just for processing etc.
*   Charlie: That’s reasonable. As the prototype becomes more developed we should be able to make better guesses.
*   Erik: What delay is this?
*   Charlie: For browser->adtech, its up in the air, but 1 hour seems like it could be reasonable. Processing should be on the order of hours. 
*   Erik: You need to get enough events in order for the aggregate output to be meaningful. Need to tradeoff between waiting and having more budget, or aggregating sooner and getting more noise.
*   Charlie: You will get the same noise either way.
*   Erik: But if you aggregated each report the output would be meaningless.
*   Charlie: Shorter measurements can maybe look at top-level things. Whereas longer measurements can measure more granular information.
*   Michael: In the model where you take a batch of records and send them though the aggregate server, is not the only way. A piece of aggregation machinery which you feed reports to one at a time, and does a pub-sub model that tell you your privacy budget, is not what we proposed here. However from a privacy perspective this is compatible with the Privacy sandbox and the privacy characteristics here. But this comes down to having another server side component. Plausible that there is another system which keeps track of the budget remaining in your system and gives reports this signal.
*   Brian: Two use cases, trying to track how much of something is happening…
*   Viraj: Is there an explainer for that kind of service which solves these problems?
*   Michael: No, we haven;t publisher a design that describes such a service. But it is not in opposition with the goals or design choices made thus far.
*   Charlie: The biggest tradeoff here is a security vs utility model, if such a thing is hard to implement in an MPC kind of way. If a sole entity needs to track the budgets, this is different from the zero knowledge MPC protocols.
*   Michael: Seems reasonable to compute in MPC, where this things are stored as secret shares. Abstractly, not sure why this couldn’t be part of an aggregate reporting story.
*   Viraj: Perhaps we can have some discussion on the priority here if some party publishes an explainer. 
*   Charlie: There is an existing issue for this.
*   Erik: Is the idea that while we are within this budget, having a different kind of computation may be okay? Could we, say, train a shallow neural network within the privacy budget?
*   Charlie: In my mind that is with the philosophical boundaries, if you have a proposal which does neural net training with the MPC model. Not confident that we have great techniques to do efficient training at this scale.
*   Michael: The privacy calculations are substantially different given the different output.
*   Charlie: This s designed to be a generic API, so hardcoding a particular learning mechanism may not work for everyone. One thing to keep in mind if adding a proposal.
*   Erik: Encrypted reports get sent to adtech company who have contracts with an MPC consortium. Should adtechs be able to innovate a solution that works for them?
*   Charlie: Two things: do you have the ability to draft the correct message on the client? Needs to be generic enough. (2) what is the mechanism in which these MPC systems determine what computations are available. Perhaps the MPC servers will have their own standards that can be innovated on.
*   Erik: Assuming this is within the privacy budget.
*   Charlie: The browser still needs to trust that the servers are doing the right thing privacy wise.
*   Robert: Thinking about targeting that the sandbox will support along with measurement. What type of incentives will the measurement mechanisms create? Measuring last click pushed things towards retargeting? Is this gameable to the point that it will push spend towards specific targeting approaches?
*   Charlie: Haven’t thought about the case where measurement solutions push people towards fledge vs floc as an example.
*   Robert: It should be possible to game out how parties will try to look good to the attribution model.
*   Charlie: Specifically this is referring to the last click model.
*   Robert. Certainly first click vs last click influences this. Will measurement have an impact on who succeeds overall? Will solutions benefit some companies over other?
*   Charlie: Interesting point to consider, interested in feedback on the best way to happily marry all of these requirements. As is we did last click because it seemed the most useful and fit within goals. Other ideas welcome. Maybe could do something with the conversion worklet, for out attribution actually happens. We’re interested in providing the API that’s most useful.
*   Robert: You might want to think about building FLoC capabilities as being significantly less important than retargeting, given that the way measurement is currently envisaged is likely to prefer retargeting [EDITED by Robert at Eric’s request]