# Attribution Reporting API

March 8, 2021

Meet link: [https://meet.google.com/jnn-rhxv-nsy](https://meet.google.com/jnn-rhxv-nsy)

Use Google meet “Raise hand” for queuing


# Agenda



*   Introductions
*   Scribe volunteer
*   Aggregation key discussion ([slides](https://docs.google.com/presentation/d/1mMmh8OOAM-YTGHUrH_bak3wfnPE78dRMr0e_ugB-ewg))
    *   Key domain size, efficiency / privacy
    *   Thresholds vs. fixed vector
*   [Attribution worklet](https://github.com/WICG/conversion-measurement-api/issues/114) 
*   Any other business


# Attendees — please sign yourself in!



1. Charlie Harrison (Google)
2. Ben Savage (Facebook)
3. Brendan Riordan-Butterworth (IAB Tech Lab / eyeo GmbH)
4. Erik Taubeneck (Facebook)
5. Erik Anderson (Microsoft)
6. Paul Marcilhacy (Criteo)
7. John Delaney (Google)
8. Chris Evans (NextRoll)
9. Andrew Pascoe (NextRoll)
10. Ryan Avecilla (Neustar)
11.  Maud Nalpas (Google Chrome)
12. Michael Kleber (Google Chrome)
13. Marshall Vale (Google Chrome)
14. Danny Rojas (Google Chrome)
15. Vincent Grosbois (Criteo)
16. Larissa Licha (NextRoll)
17. Robert Stratton (Neustar)
18. Andrew Knox (Facebook)
19. Daniel Smedley (Booking.com)
20. Nicolas Chrysanthos (Criteo)
21. Matt Zambelli (Neustar)
22. Brian May (dstillery)
23. Mert Zeytinci (Booking.com)
24. Basile Leparmentier (Criteo)
25. Joao Natali (Neustar)
26. Przemyslaw Iwanczak (RTB House)
27. 
28. 


# Notes

Brendan is Scribe

Charlie Harrison - I have a short slide deck on Aggregation Key Discussion.  Feel free to interrupt.  

CH - starts sharing the document.  

CH - Aggregate Computations that can support differential privacy.  We want to support this goal (Differential Privacy).  How can we provide Diff Priv output through the API?  This has been discussed a bit (internally and externally), but I wanted to go through examples to solicit feedback on what people think.  

CH - these computations really affect some of the MPC protocols (2 party protocols) what exactly is being computed really matters.  

CH - First example, really simple, vector with fixed length.  Slide has more details.  Agg Key is the index into the vector.  Maybe cach user has an input, and the algo returns a sum to you.  

CH - this is probably the simplest way to support a histogram style output. You’re able to index into each vector, but it has a bunch of downsides.  

CH - Very large sparse domains is a bunch of noise around 0 (like, at the TB scale!). There are challenges in compressing dense space into something smaller.  But it does have a bunch of benefits, like simplicity, known techniques in MPC, no thresholding.  Thresholding does introduce one-sided bias, while this doesn’t.  

CH - a bit more fuzzy benefit, small domains get a privacy bonus.  This is not measurable directly, but affects privacy leakage.  PCM is an example of this technique, where the output domain is 12 bits, and you have to specify the keys in a certain way.  But you can’t really ID a user in a domain, so there’s benefit.  

Ben Savage - Are you suggesting a 1 row in / 1 row out application of noise?

CH - conceptually, the input would be 1 vector per user, a 1 hot vector, a bunch of 0s and the one thing you’re adding. Then 1 row of noise is added.  

BS - the row that you’ve drawn there, is it a summation across multiple outputs?  

CH - This vector is meant to rep the final output of the API.  The slide shows number of users per cell on the vector.  

BS - Help me think about what these things might represent?

CH - Index 0 might represent something as complicated as “campaign for this brand at this geo”, a dense encoding of all 

Basile leparmentier - Indexes  Number of displays, number of clicks, for campaign? 

CH - you  could use this for a large domain - you could take a hash of the domain and index into one of the slots, but your output would be proportional to the scope of your domain.  This one is not very ergonomic.  

Erik Taubeneck - Sum and Mean, which do you mean?  

CH - Sum, the noise would have mean 0 probably?  I don’t think mean is too difficult, esp if you know the size of the inputs.  The explainer published before (reporting origin), you probably have this information (size of inputs) in the clear without 

ET - The D is the number of entries, not the max value.  You would need to know max value to determine the noise.  

CH - Some aspects are skipped in this simple example.  If you wanted to sum up more things, you could have the user contribute more than just a binary value.  

Andrew Pascoe - What’s teh value, instead of just sending a string?  Even if you’re not going to collide because of large space?  

CH - benefits are the last 3 bullets.  The main privacy benefit for consumers of the API is potentially the thresholding piece.  If your domain is really large and we’re (API) is not giving all answers, then there have to be omissions.  There might be other fuzzy benefits, like it being faster and more efficient, but that’s implementation details.  

ET - Does this have to be fixed, globally?  

CH - Probably not, something about preconfiguration.  Per advertiser possibly?  The size can vary based on advertiser needs. 

Brian May - have you considered using a hierarchical data model?

CH - 2 slides from now.  

CH - Next technique “Sparse Vector Technique”.  It’s the same as prev, except that your mechanism doesn’t need to know the entire output domain, or the output domain can be so large to be infeasible to enumerate.  

CH - imagine the same noisy vector as previously, except the input doesn’t have any of the 0 entries, basically.  Thresholding step looks after you apply noise to all non-zero, and threshold removes all the things that are too small.  

CH - this domain could go on much larger.  But, we need to introduce thresholding to protect the presence or absence of any one key.  

CH - I have a link in the deck for some proofs around this.  

CH - It’s ergonomic for consumers, but it has downsides, like the thresholding step.  Probably more complicated to support in something like MPC step.

BM - This is being presented like “pick 1”, is there possibility of supporting multiple?

CH - Yeah, mainly looking for feedback right now.  Due to tight timelines, stack rank, figure out what works most well for initial and maybe everything else will happen.  There’s a lot of work to support everything.  I can see use cases for all of these.  

CH - Last slide, the hierarchical approach.  A lot of different ways this could be implemented.  You’re still in the region where the output domain is a vector of fixed size, but you can create a hierarchy based on ID prefixes.  A very large domain without a need to define everything, you could query on prefixes only.  

CH - Green represents what you can query, and doing iterative query, each query looks just like the first slide.  Based on the results of that query, you could decide which entry in the child vector that you want to query.  

CH - There’s ability to prune the search.  The mechanism for the API could enforce threshold, and the querying could also.  

CH - Iteratively querying portions of each row, like you could say only look at half, sortof like a range query.  You can specify prefixes or other hierarchy.  The client of the API can be selective, and more efficient in the query.  There would have to be some privacy budgeting.  Splitting noise across the different levels, so you’d need to tolerate more noise.  

CH - Avoids the problem of the 

BM - Seeing a graph database here, would you allow the creation of a graph database, and ensuring that the results are OK / privacy preserving?

CH - the biggest issue is how to compute the outputs in a multiple worker query environment.  That’s something that’s really flexible!  And useful!  But harder to make privacy preserving.  Trying to implement this in the multi-helper PC model is risk of being expensive and risky!

BM - Starting simple is good, building up to something more complex is good.  

ET - MPS is notoriously slow.  And expensive.  Imagining you’re not precomputing all of this.  Esp with privacy budget!  The consumer is, this is going to be a slow process.  It’s a clever way to do it, but it might be very consuming of resources.  

CH - There are two ways conceptually.  The way outlined is an interaction with the “consumer”, not necessary a human, but a rules based way of iterating through this tree.  Like as simple as “a threshold of 1”.  There are other ways to do this, like, within the MPC system itself, with interactions between the helpers.  One challenge is hiding results from helpers.  It would be hard to make this actually zero-knowledge to the helpers.  

ET - If it’s programmable, then it makes sense to run it on the client.  

CH - Not sure what the reqs on the helpers will be, but making them zero-knowledge would be a nice property to have them.  

ET - Was going to suggest these run at the MPC, so that you could apply privacy budget at the end, instead of each layer.  

BL - Criteo proposed an API that would stop after a budget was reached.  

CH - Yeah, k-anonymity could be a criteria.  This is analogous.  

BS - Would this only support 1 particular order for traversing the hierarchy?  Ad manager needs to support multiple directions.  There are combos that are supported, and that aren’t supported.  If I have to structure the prefixes in a particular order, 

BM - Graph Database supports this, 

CH - Good point.  Digging into this.  Hierarchy is useful in general, but… Ads data is not particularly hierarchical.  You can mix these things around, and that’s a challenge.  I’m thinking about it: you could have multiple hierarchies (conceptual hierarchies) within the structure.  And each conversion / attribution report could contribute multiple times.  Left is Age / Geo, right is Geo / Age.  Each report could contribute twice, you could get rollups for both.  It’s not exactly what you want, since each would have to be specified at conversion time, you wouldn’t be able to do this late binding / at query time.  

BS - It’s OK not to support late binding!  We had to have a fixed set of agg keys ahead of time.  

CH - The difference, in your system you can mix and match, but in this case you wouldn’t be able to do at will.  Unless a ton of buckets were contributed at the same time, but this increases the costs, as the vectors increase in size.  

BM - Graph Database, 2 benefits.  (1) You don’t know what the structure will be ahead of time, and you don’t know what you’re going to get, and you don’t know what you want to query.  (2) Don’t have to figure out all the stuff before you run the campaign. 

CH - Flexibility is great!  But it’s hard.  Ad systems have this flexibility today.  The biggest challenge to offering new version is how things are computed.  

CH - Adjust costs based on sizes is a good point!  

CH - A bit last minute - I’ll try to file an issue on this and link the deck. This feedback today has been useful, but we’re looking to see what people prefer.  Try to design some of these protocols to see what’s satisfiable.  

BL - Thank you for this!  I cannot stop noticing that this is a huge revolution.  We are not currently working with data like this, need multi-party computation and differential privacy, all at once!  Can we start with a centralized server that is first k-anonymous?  I am scare for my company’s business.  This has to be used for billing, fraud, learning models.  Understand that this is privacy preserving, but.

BL - This is shaking the core ways of doing business for a multibillion dollar business.  Can we go there incrementally?  This is so much to go for, is there something that is an incremental step?  We want it quite slow, and recognize that you want it faster.  It’s great to have the long term plan, but I really feel that this a single step change is going to be really hard.  

CH - Totally acknowledge that this is a big change.  In terms of incremental step, this is the intention of the event level APIs.  We tried to design something where all the privacy power was applied in the browser.  I’m hoping that if we want to see this as a long term, that is great.  We do expect to see a lot of use cases that can use the event level APIs now, and that the data there is of relatively good quality.  

CH - I definitely appreciate the feedback. This is going to be a huge change.  

ET - One other idea in this area.  If, in the first case, where we want to sparsify the vector, we can add noise to which element we are adding to.  Add a little bit of noise to the hash value, and achieve the same result as thresholding.  Adding noise to a spatial representation will still capture some (not all) of the data, and still provide the privacy guarantees.  

CH - Would the client’s input data be noised independently?  

ET - no, at the MPC.  The pub would put in key, the advertiser would put in a value, all of these could get shifted around a little bit.  If there is semantic meaning to that key/value, such that you could get information from it.  Shifting it around doesn’t change the meaning absolutely, might become useful for model training, and still support sparsifying this dense thing.  

CH - Need to think about this more.  Biggest concern is like, if you aren’t doing this per… If you’re doing this per key on the input? So for each key we’re applying random shift if you?  Big concern is that in a really sparse place, a present key would be unlikely bump into a non-present key.  The presence key would be a disclosure without high noise.  

ET - Yes, the noise would have to be proportional to size. 

CH - The noise might become less of an issue as the size of the vector gets small enough that it’s easier to handle.  

ET - You would need a lot of noise to make this work…

BM - High noise value would make it hard to use the data.  

ET - Might be best for model training, rather than anything else.  

CH - 7 minutes left, let’s go quick!  

CH - Related to first topic.  What has been challenging to think about is how the API surface works, especially when the keys are attribution plus click.  The existing API made it really hard to specify an index into a million-sized vector.  

CH - There are a few other features that fit into this model.  Like programmatic control.  Applies to Ben’s previously filed, the presence or absence of report was meaningful.  In this, the content of the report is interesting as well.  

CH - The way this would work - adding a new context ID into the anchor tag, arbitrary string, limits only disk considerations on the user.  Conversion / Attribution trigger time, an idea of Worklet (like TURTLEDOVE / FLEDGE), isolated execution environment, where trigger and context are in the clear.  

CH - Maybe we can allow to contribute to multiple buckets.  

CH - key thing is that the function arguments are in the clear, like SPURFOWL.  Tried to take that idea and mash into our API a bit cleaner.  

CH - ability to return no, or NULL value.  

ET - Are we specifying the two helpers here? How do we guarantee these aren’t two ad tech domains.

CH - For an origin trial, we are exploring allowing devs to specify their own helpers. Long term, these helpers would need to be allowlisted parties trusted by the browser. Even the choice of helpers leaks information in that scenario

BS - Both the source context and trigger context do not appear in the output? 

CH - Could imagine that the bucket is the hash of (trigger + source) contexts, are implicitly allowing for them to show up in the output. 

BS - Looking at how conversion filters applies here, a single purchase event can contain items from different merchants. The trigger context could contain cart info, source context can define which merchant the campaign is running for. I could do an intersection to see what portions of the conversion are related to the campaign?

CH - Yes
